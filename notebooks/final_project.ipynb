{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "44861931",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6c57aade",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# remember to install \"LeWagon_FinalProject\" package\n",
    "from LeWagon_FinalProject.data import DataProcessor\n",
    "\n",
    "# for LDA\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "# for NMF\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import NMF\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85a3b65d",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Setup \"LeWagon_FinalProject\" Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6ecabed",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Create pyenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f400cce0",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# pyenv virtualenv LeWagon_FinalProject\n",
    "# pyenv local LeWagon_FinalProject\n",
    "# pip install --upgrade pip\n",
    "#pip install -r https://gist.githubusercontent.com/krokrob/53ab953bbec16c96b9938fcaebf2b199/raw/9035bbf12922840905ef1fbbabc459dc565b79a3/minimal_requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b34e6af7",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Install project package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31407840",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Install LeWagon_FinalProject package\n",
    "#!pip install -e ."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "551d6ee4",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### NLTK data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d004b28f",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Download nltk data\n",
    "#import nltk\n",
    "#nltk.download('wordnet')\n",
    "#nltk.download('punkt')\n",
    "#nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21c46b05",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f6e1e077",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 9)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>publication</th>\n",
       "      <th>author</th>\n",
       "      <th>date</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>17283</td>\n",
       "      <td>house republican fret winning health care suit...</td>\n",
       "      <td>New York Times</td>\n",
       "      <td>Carl Hulse</td>\n",
       "      <td>2016-12-31</td>\n",
       "      <td>2016</td>\n",
       "      <td>12</td>\n",
       "      <td>washington congressional republican new fear c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>17284</td>\n",
       "      <td>rift officer resident killing persist south br...</td>\n",
       "      <td>New York Times</td>\n",
       "      <td>Benjamin Mueller and Al Baker</td>\n",
       "      <td>2017-06-19</td>\n",
       "      <td>2017</td>\n",
       "      <td>6</td>\n",
       "      <td>bullet shell get counted blood dry votive cand...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>17285</td>\n",
       "      <td>tyrus wong ‘ bambi artist thwarted racial bias...</td>\n",
       "      <td>New York Times</td>\n",
       "      <td>Margalit Fox</td>\n",
       "      <td>2017-01-06</td>\n",
       "      <td>2017</td>\n",
       "      <td>1</td>\n",
       "      <td>walt disney bambi opened 1942 critic praised s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>17286</td>\n",
       "      <td>among death 2016 heavy toll pop music new york...</td>\n",
       "      <td>New York Times</td>\n",
       "      <td>William McDonald</td>\n",
       "      <td>2017-04-10</td>\n",
       "      <td>2017</td>\n",
       "      <td>4</td>\n",
       "      <td>death may great equalizer necessarily evenhand...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>17287</td>\n",
       "      <td>kim jong un say north korea preparing test lon...</td>\n",
       "      <td>New York Times</td>\n",
       "      <td>Choe Sang-Hun</td>\n",
       "      <td>2017-01-02</td>\n",
       "      <td>2017</td>\n",
       "      <td>1</td>\n",
       "      <td>seoul south korea north korea leader kim said ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0     id                                              title  \\\n",
       "0           0  17283  house republican fret winning health care suit...   \n",
       "1           1  17284  rift officer resident killing persist south br...   \n",
       "2           2  17285  tyrus wong ‘ bambi artist thwarted racial bias...   \n",
       "3           3  17286  among death 2016 heavy toll pop music new york...   \n",
       "4           4  17287  kim jong un say north korea preparing test lon...   \n",
       "\n",
       "      publication                         author       date  year  month  \\\n",
       "0  New York Times                     Carl Hulse 2016-12-31  2016     12   \n",
       "1  New York Times  Benjamin Mueller and Al Baker 2017-06-19  2017      6   \n",
       "2  New York Times                   Margalit Fox 2017-01-06  2017      1   \n",
       "3  New York Times               William McDonald 2017-04-10  2017      4   \n",
       "4  New York Times                  Choe Sang-Hun 2017-01-02  2017      1   \n",
       "\n",
       "                                             content  \n",
       "0  washington congressional republican new fear c...  \n",
       "1  bullet shell get counted blood dry votive cand...  \n",
       "2  walt disney bambi opened 1942 critic praised s...  \n",
       "3  death may great equalizer necessarily evenhand...  \n",
       "4  seoul south korea north korea leader kim said ...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dp = DataProcessor(csv_path='../raw_data/', csv_name='articles1')\n",
    "\n",
    "#  Process the data, in case it was not processed\n",
    "#dp.process_data()\n",
    "\n",
    "# Load the data processed\n",
    "df = dp.load_dataset_processed()\n",
    "\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d4006d5",
   "metadata": {},
   "source": [
    "## Base Model - NMF and LDA using sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a1434cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://kitt.lewagon.com/camps/582/lectures/05-ML%2F10-Natural-Language-Processing#source\n",
    "# https://kitt.lewagon.com/camps/582/lectures/06-Deep-Learning%2F05-Natural-Language-Processing#source\n",
    "# https://shravan-kuchkula.github.io/topic-modeling/#interactive-data-visualization-showing-relation-between-clustering-sentiment-and-topics\n",
    "\n",
    "#count_vectorizer = CountVectorizer()\n",
    "#tfidf_vectorizer = TfidfVectorizer()\n",
    "\n",
    "#count_vectorizer = CountVectorizer(min_df=10, max_df=0.95, ngram_range=(1,1), stop_words='english')\n",
    "#tfidf_vectorizer = TfidfVectorizer(min_df=10, max_df=0.95, ngram_range=(1,1), stop_words='english')\n",
    "\n",
    "count_vectorizer = CountVectorizer(min_df=10, max_df=0.95, ngram_range=(2,2), stop_words='english')\n",
    "tfidf_vectorizer = TfidfVectorizer(min_df=10, max_df=0.95, ngram_range=(2,2), stop_words='english')\n",
    "\n",
    "# calculate the feature matrix\n",
    "feature_matrix = count_vectorizer.fit_transform(df['content'].astype('U').values)\n",
    "tfidf_feature_matrix = tfidf_vectorizer.fit_transform(df['content'].astype('U').values)\n",
    "\n",
    "print(feature_matrix.shape)\n",
    "print(tfidf_feature_matrix.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b9b6908",
   "metadata": {},
   "source": [
    "### NMF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07da63e0",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "525af358",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#####################################\n",
    "## Utility functions to help with NMF\n",
    "# Code adapted from Sarkar text book\n",
    "#####################################\n",
    "\n",
    "# get topics with their terms and weights\n",
    "def get_topics_terms_weights(weights, feature_names):\n",
    "    feature_names = np.array(feature_names)\n",
    "    sorted_indices = np.array([list(row[::-1]) for row in np.argsort(np.abs(weights))])\n",
    "    sorted_weights = np.array([list(wt[index]) for wt, index in zip(weights, sorted_indices)])\n",
    "    sorted_terms = np.array([list(feature_names[row]) for row in sorted_indices])\n",
    "\n",
    "    topics = [np.vstack((terms.T, term_weights.T)).T for terms, term_weights in zip(sorted_terms, sorted_weights)]\n",
    "\n",
    "    return topics\n",
    "\n",
    "\n",
    "# prints components of all the topics\n",
    "# obtained from topic modeling\n",
    "def print_topics_udf(topics, total_topics=1,\n",
    "                     weight_threshold=0.0001,\n",
    "                     display_weights=False,\n",
    "                     num_terms=None):\n",
    "\n",
    "    for index in range(total_topics):\n",
    "        topic = topics[index]\n",
    "        topic = [(term, float(wt))\n",
    "                 for term, wt in topic]\n",
    "        #print(topic)\n",
    "        topic = [(word, round(wt,2))\n",
    "                 for word, wt in topic\n",
    "                 if abs(wt) >= weight_threshold]\n",
    "\n",
    "        if display_weights:\n",
    "            print('Topic #'+str(index+1)+' with weights')\n",
    "            print(topic[:num_terms]) if num_terms else topic\n",
    "        else:\n",
    "            print('Topic #'+str(index+1)+' without weights')\n",
    "            tw = [term for term, wt in topic]\n",
    "            print(tw[:num_terms]) if num_terms else tw\n",
    "\n",
    "# prints components of all the topics\n",
    "# obtained from topic modeling\n",
    "def get_topics_udf(topics, total_topics=1,\n",
    "                     weight_threshold=0.0001,\n",
    "                     num_terms=None):\n",
    "\n",
    "    topic_terms = []\n",
    "\n",
    "    for index in range(total_topics):\n",
    "        topic = topics[index]\n",
    "        topic = [(term, float(wt))\n",
    "                 for term, wt in topic]\n",
    "        #print(topic)\n",
    "        topic = [(word, round(wt,2))\n",
    "                 for word, wt in topic\n",
    "                 if abs(wt) >= weight_threshold]\n",
    "\n",
    "        topic_terms.append(topic[:num_terms] if num_terms else topic)\n",
    "\n",
    "    return topic_terms\n",
    "\n",
    "def getTermsAndSizes(topic_display_list_item):\n",
    "    terms = []\n",
    "    sizes = []\n",
    "    for term, size in topic_display_list_item:\n",
    "        terms.append(term)\n",
    "        sizes.append(size)\n",
    "    return terms, sizes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ed28f1e",
   "metadata": {},
   "source": [
    "#### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a74f92f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_topics = 30\n",
    "num_of_terms = 9\n",
    "#nmf = NMF()\n",
    "nmf = NMF(n_components=number_of_topics, random_state=43,  alpha=0.1, l1_ratio=0.5)\n",
    "nmf_output = nmf.fit_transform(tfidf_feature_matrix)\n",
    "\n",
    "nmf_feature_names = tfidf_vectorizer.get_feature_names()\n",
    "nmf_weights = nmf.components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65780497",
   "metadata": {},
   "outputs": [],
   "source": [
    "topics = get_topics_terms_weights(nmf_weights, nmf_feature_names)\n",
    "#print_topics_udf(topics, total_topics=number_of_topics, num_terms=num_of_terms, display_weights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "342fc363",
   "metadata": {},
   "outputs": [],
   "source": [
    "topics_display_list = get_topics_udf(topics, total_topics=number_of_topics, num_terms=num_of_terms)\n",
    "print(len(topics_display_list[0]))\n",
    "#topics_display_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7119fa28",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_columns = ['topic']\n",
    "for i in range(num_of_terms):\n",
    "    topic_columns.append(f'term_{i}')\n",
    "    topic_columns.append(f'weight_{i}')\n",
    "\n",
    "df_topics = pd.DataFrame(columns=topic_columns)\n",
    "for i in range(number_of_topics):\n",
    "    new_topic = {} \n",
    "    new_topic['topic'] = f'topic_{i}'\n",
    "    for j in range(num_of_terms):\n",
    "        new_topic[f'term_{j}'] = topics_display_list[i][j][0]\n",
    "        new_topic[f'weight_{j}'] = topics_display_list[i][j][1]\n",
    "    #print(new_topic)\n",
    "    df_topics = df_topics.append(new_topic, ignore_index=True)\n",
    "    #break\n",
    "#df_topics.to_csv('../raw_data/NMFResults.csv', header=True, index=False, encoding='utf-8')\n",
    "df_topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bcdd295",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "for i in range(number_of_topics):\n",
    "    terms, sizes = getTermsAndSizes(topics_display_list[i])\n",
    "\n",
    "    num_top_words = num_of_terms\n",
    "    fontsize_base = num_of_terms / np.max(sizes)\n",
    "\n",
    "    num_topics = 1\n",
    "\n",
    "    for t in range(num_topics):\n",
    "        fig, ax = plt.subplots(1, num_topics, figsize=(6, 12))\n",
    "        plt.ylim(0, num_top_words + 1.0)\n",
    "        plt.xticks([])\n",
    "        plt.yticks([])\n",
    "        plt.title('Topic #{}'.format(t))\n",
    "\n",
    "        for i, (word, share) in enumerate(zip(terms, sizes)):\n",
    "            word = word + \" (\" + str(share) + \")\"\n",
    "            plt.text(0.3, num_top_words-i-1.0, word, fontsize=fontsize_base*share)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    sns.set(rc={'axes.facecolor':'cornflowerblue', 'figure.facecolor':'cornflowerblue'})\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0a1da42",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fa49d2c",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "parei aqui\n",
    "# Instantiate the LDA model\n",
    "'''lda_model = LatentDirichletAllocation(n_components=30, max_iter=100, learning_method='online', random_state=43,\n",
    "                                     batch_size=128, evaluate_every=-1, n_jobs=-1)\n",
    "\n",
    "# fit transform the feature matrix\n",
    "lda_output = lda_model.fit_transform(feature_matrix)\n",
    "\n",
    "# display the lda_output and its shape\n",
    "print(lda_output)\n",
    "print(lda_output.shape)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "897db2bf",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# print log-likelihood\n",
    "#print(\"Log likelihood: \", lda_model.score(feature_matrix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62c1db40",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# print perplexity\n",
    "#print(\"Perplexity: \", lda_model.perplexity(feature_matrix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cc3482e",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Define Search Param\n",
    "#search_params = {'n_components': [2, 3, 4, 5, 10, 15, 20, 25], 'learning_decay': [.5, .7, .9]}\n",
    "\n",
    "#search_params = {'n_components': [30], 'learning_decay': [.5, .7, .9]}\n",
    "\n",
    "search_params = {'n_components': [30], 'learning_decay': [.7]}\n",
    "\n",
    "# Init the model\n",
    "lda = LatentDirichletAllocation()\n",
    "\n",
    "# Init Grid Search class\n",
    "model = GridSearchCV(lda, search_params)\n",
    "\n",
    "model.fit(feature_matrix)\n",
    "best_lda_model = model.best_estimator_\n",
    "print(\"Best model's params: \", model.best_params_)\n",
    "print(\"Best log likelihood score: \", model.best_score_)\n",
    "print(\"Model perplexity: \", best_lda_model.perplexity(feature_matrix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5203f615",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df_cv_results = pd.DataFrame(model.cv_results_)\n",
    "#df_cv_results.to_csv('../raw_data/LDAGridSearchResults.csv', header=True, index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40ab39a6",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "sns.pointplot(x='param_n_components', y='mean_test_score', hue='param_learning_decay', data=df_cv_results)\n",
    "sns.set(rc={'axes.facecolor':'cornflowerblue', 'figure.facecolor':'cornflowerblue'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91f839e9",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "best_lda_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfc0ad1d",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Create a document to topic matrix\n",
    "lda_output = best_lda_model.transform(feature_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc6d5a34",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# column names\n",
    "topicnames = ['Topic_' + str(i) for i in range(best_lda_model.n_components)]\n",
    "\n",
    "# index names\n",
    "clean_content_text = df['content'].to_list()\n",
    "docnames = ['Doc_' + str(i) for i in range(len(clean_content_text))]\n",
    "\n",
    "# create a dataframe\n",
    "df_document_topic = pd.DataFrame(np.round(lda_output,2), columns=topicnames, index=docnames)\n",
    "\n",
    "df_document_topic.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3155b084",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# dominant topic\n",
    "df_document_topic['dominant_topic'] = np.argmax(df_document_topic.values, axis=1)\n",
    "df_document_topic.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32956d78",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "sns.countplot(df_document_topic.dominant_topic)\n",
    "sns.set(rc={'axes.facecolor':'cornflowerblue', 'figure.facecolor':'cornflowerblue'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97737466",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#components_ contains the word to topic matrix\n",
    "best_lda_model.components_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd3da398",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# check the shape\n",
    "feature_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c5462c5",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Topic - Keyword matrix\n",
    "df_topic_keywords = pd.DataFrame(best_lda_model.components_)\n",
    "\n",
    "# assign column and index\n",
    "df_topic_keywords.columns = count_vectorizer.get_feature_names()\n",
    "df_topic_keywords.index = topicnames\n",
    "\n",
    "\n",
    "# check the head\n",
    "#df_topic_keywords.iloc[:,:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d27cf35b",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Get the top 15 keywords from each topic\n",
    "# Show top n keywords for each topic\n",
    "def show_topics(vectorizer=count_vectorizer, lda_model=best_lda_model, n_words=20):\n",
    "    keywords = np.array(vectorizer.get_feature_names())\n",
    "    topic_keywords = []\n",
    "    for topic_weights in lda_model.components_:\n",
    "        top_keyword_locs = (-topic_weights).argsort()[:n_words]\n",
    "        topic_keywords.append(keywords.take(top_keyword_locs))\n",
    "    return topic_keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24b48b64",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "topic_keywords = show_topics(count_vectorizer, best_lda_model, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af7ef410",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#topic_keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbf1ffb6",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df_topic_keywords = pd.DataFrame(topic_keywords)\n",
    "df_topic_keywords.columns = ['Word '+str(i) for i in range(df_topic_keywords.shape[1])]\n",
    "df_topic_keywords.index = ['Topic '+str(i) for i in range(df_topic_keywords.shape[0])]\n",
    "#df_topic_keywords.to_csv('../raw_data/LDAResults.csv', header=True, index=False, encoding='utf-8')\n",
    "df_topic_keywords"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9df3bf71",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de550a5a",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "parou aqui\n",
    "import tensorflow as tf\n",
    "import transformers\n",
    "from transformers import BertTokenizer\n",
    "# https://huggingface.co/transformers/pretrained_models.html\n",
    "# https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
    "# bert-large-uncased\n",
    "# !pip install transformers\n",
    "\n",
    "# text = df['content'].to_list()\n",
    "text = df['content'].values\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-large-uncased', do_lower_case=True)\n",
    "\n",
    "input_ids = []\n",
    "lengths = []\n",
    "\n",
    "for txt in text:\n",
    "    enconded_sent = tokenizer.encode(str(txt),\n",
    "                                     add_special_tokens=True,\n",
    "                                     return_tensors='tf',\n",
    "                                     max_length=512,\n",
    "                                     truncation=True\n",
    "                                    )\n",
    "    input_ids.append(enconded_sent)\n",
    "    lengths.append(len(enconded_sent[0]))\n",
    "\n",
    "print(len(input_ids))\n",
    "print(len(lengths))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25cc7ac9",
   "metadata": {},
   "source": [
    "## BERT topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e1fccb24",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'DataProcessor' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_13425/3417234161.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mbertopic\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBERTopic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mdp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataProcessor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcsv_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'../raw_data/'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcsv_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'articles1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'DataProcessor' is not defined"
     ]
    }
   ],
   "source": [
    "# pip install bertopic\n",
    "# Topic Modeling with BERT Transformers\n",
    "# https://www.youtube.com/watch?v=TLPmlVeEf1k\n",
    "# https://maartengr.github.io/BERTopic/index.html\n",
    "# https://maartengr.github.io/BERTopic/tutorial/visualization/visualization.html\n",
    "# https://maartengr.github.io/BERTopic/tutorial/topicreduction/topicreduction.html\n",
    "# https://www.atoti.io/topic-modeling-on-twitter-using-sentence-bert/\n",
    "# https://hdbscan.readthedocs.io/_/downloads/en/stable/pdf/\n",
    "\n",
    "from bertopic import BERTopic\n",
    "\n",
    "dp = DataProcessor(csv_path='../raw_data/', csv_name='articles1')\n",
    "df = dp.load_dataset()\n",
    "\n",
    "docs = df['content'].values\n",
    "\n",
    "topic_model = BERTopic(min_topic_size=150, language='english', calculate_probabilities=True)\n",
    "#topic_model = BERTopic(language='english', calculate_probabilities=True)\n",
    "topics, _ = topic_model.fit_transform(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6307db02",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_freq = topic_model.get_topic_freq()\n",
    "outliers = topic_freq['Count'][topic_freq['Topic']==-1].iloc[0]\n",
    "print(f'{outliers} documents have not been classified')\n",
    "print(f'The other {topic_freq[\"Count\"].sum() - outliers} documents are {topic_freq[\"Topic\"].shape[0]-1} topics')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93bd667c",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_freq.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11d2b805",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'There are {topic_freq[\"Count\"].iloc[1]} documents that are talking about topic ID {topic_freq[\"Topic\"].iloc[1]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3ffa6e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_model.get_topic(topic_freq[\"Topic\"].iloc[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5549695c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_topic_info = topic_model.get_topic_info()\n",
    "\n",
    "#df_topic_info.to_csv('../raw_data/BERTopicInfo.csv', header=True, index=False, encoding='utf-8')\n",
    "df_topic_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e6a1a3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "topics = topic_model.get_topics()\n",
    "number_of_topics = len(topics)-1\n",
    "num_of_terms = len(topics[0])\n",
    "\n",
    "topic_columns = ['topic']\n",
    "for i in range(num_of_terms):\n",
    "    topic_columns.append(f'term_{i}')\n",
    "    topic_columns.append(f'weight_{i}')\n",
    "\n",
    "df_topics = pd.DataFrame(columns=topic_columns)\n",
    "for i in range(-1,number_of_topics):\n",
    "    new_topic = {} \n",
    "    new_topic['topic'] = topic_model.topic_names[i] #f'topic_{i}'\n",
    "    for j in range(num_of_terms):\n",
    "        new_topic[f'term_{j}'] = topics[i][j][0]\n",
    "        new_topic[f'weight_{j}'] = topics[i][j][1]\n",
    "    df_topics = df_topics.append(new_topic, ignore_index=True)\n",
    "\n",
    "#df_topics.to_csv('../raw_data/BERTopicResults.csv', header=True, index=False, encoding='utf-8')\n",
    "df_topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0f2566f",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_model.visualize_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdadec92",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_model.visualize_barchart()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86b980ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_model.visualize_heatmap()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49600f5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#topic_model.visualize_topics_over_time()\n",
    "#topic_model.topics_per_class(docs, topics, classes)\n",
    "print(topic_model.topic_sim_matrix.shape)\n",
    "topic_model.topic_sim_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e8d6e89",
   "metadata": {},
   "outputs": [],
   "source": [
    "similar_topics, similarity = topic_model.find_topics(topic_model.topic_names[43], top_n=3)\n",
    "print(topic_model.topic_names[similar_topics[0]])\n",
    "topic_model.get_topic(similar_topics[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "710835de",
   "metadata": {},
   "outputs": [],
   "source": [
    "similar_topics, similarity = topic_model.find_topics('health', top_n=5)\n",
    "print(topic_model.topic_names[similar_topics[0]])\n",
    "print(topic_model.get_topic(similar_topics[0]))\n",
    "print(topic_model.topic_names[similar_topics[1]])\n",
    "print(topic_model.get_topic(similar_topics[1]))\n",
    "print(topic_model.topic_names[similar_topics[2]])\n",
    "print(topic_model.get_topic(similar_topics[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aad46f69",
   "metadata": {},
   "outputs": [],
   "source": [
    "topics = topic_model.get_topics()\n",
    "number_of_topics = len(topics)-1\n",
    "top_similarity = 11\n",
    "\n",
    "topic_columns = ['topic']\n",
    "for i in range(top_similarity):\n",
    "    topic_columns.append(f'topic_{i}')\n",
    "    topic_columns.append(f'weight_{i}')\n",
    "\n",
    "df_topics = pd.DataFrame(columns=topic_columns)\n",
    "for i in range(-1,number_of_topics):\n",
    "    new_topic = {} \n",
    "    new_topic['topic'] = topic_model.topic_names[i] \n",
    "    similar_topics, similarity = topic_model.find_topics(topic_model.topic_names[i], top_n=top_similarity)\n",
    "    for j in range(top_similarity):\n",
    "        new_topic[f'topic_{j}'] = topic_model.topic_names[similar_topics[j]]\n",
    "        new_topic[f'weight_{j}'] = round(similarity[j], 4)\n",
    "    df_topics = df_topics.append(new_topic, ignore_index=True)\n",
    "\n",
    "#df_topics.to_csv('../raw_data/BERTopicSimilarity.csv', header=True, index=False, encoding='utf-8')\n",
    "df_topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbf2a77c",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d205be9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def correlation_matrix_to_df(df_corr):\n",
    "    list_done = []\n",
    "    lits_item1 = []\n",
    "    lits_item2 = []\n",
    "    list_corr = []\n",
    "\n",
    "    for k in range(1,df_corr.shape[1]):\n",
    "        for i, j in df_corr.iterrows():\n",
    "            #if (df_corr.columns[k] != j[0]) and (j[0] not in list_done):\n",
    "            if (j[0] not in list_done):\n",
    "                lits_item1.append(df_corr.columns[k])\n",
    "                lits_item2.append(j[0])\n",
    "                list_corr.append(j[k])\n",
    "        list_done.append(df_corr.columns[k])\n",
    "\n",
    "    corr_dict = {'ITEM1': lits_item1,\n",
    "                 'ITEM2': lits_item2,\n",
    "                 'CORR': list_corr}\n",
    "    df_res = pd.DataFrame(corr_dict)\n",
    "    df_res = df_res.sort_values(by='CORR', ascending=False).copy()\n",
    "    df_res.reset_index(inplace=True,drop=True)\n",
    "    return df_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aacda4b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_matrix = topic_model.topic_sim_matrix\n",
    "\n",
    "topics = topic_model.get_topics()\n",
    "number_of_topics = len(topics)-1\n",
    "\n",
    "topic_columns = ['topic']\n",
    "for i in range(-1,number_of_topics):\n",
    "    topic_columns.append(topic_model.topic_names[i])\n",
    "\n",
    "df_similarity = pd.DataFrame(columns=topic_columns)\n",
    "for i in range(-1,number_of_topics):\n",
    "    new_topic = {}\n",
    "    new_topic['topic'] = topic_model.topic_names[i]\n",
    "    similar_topics, similarity = topic_model.find_topics(topic_model.topic_names[i], top_n=top_similarity)\n",
    "    for j in range(-1,number_of_topics):\n",
    "        new_topic[topic_model.topic_names[j]] = round(corr_matrix[i,j], 4)\n",
    "    df_similarity = df_similarity.append(new_topic, ignore_index=True)\n",
    "\n",
    "df_similarity.to_csv('../raw_data/BERTopicSimilarity.csv', header=True, index=False, encoding='utf-8')\n",
    "df_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18e3b028",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_similarity_simpl = correlation_matrix_to_df(df_similarity)\n",
    "#df_similarity_simpl.to_csv('../raw_data/BERTopicSimilaritySimpl.csv', header=True, index=False, encoding='utf-8')\n",
    "df_similarity_simpl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6115892e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "model = SentenceTransformer('distilbert-base-nli-stsb-mean-tokens')\n",
    "embeddings = model.encode(docs)\n",
    "embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb8336c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_most_relevant_documents(cluster_id, condensed_tree):\n",
    "          \n",
    "    assert cluster_id > -1, \"The topic's label should be greater than -1!\"\n",
    "        \n",
    "    raw_tree = condensed_tree._raw_tree\n",
    "    \n",
    "    # Just the cluster elements of the tree, excluding singleton points\n",
    "    cluster_tree = raw_tree[raw_tree['child_size'] > 1]\n",
    "    \n",
    "    # Get the leaf cluster nodes under the cluster we are considering\n",
    "    leaves = hdbscan.plots._recurse_leaf_dfs(cluster_tree, cluster_id)\n",
    "    \n",
    "    # Now collect up the last remaining points of each leaf cluster (the heart of the leaf)\n",
    "    result = np.array([])\n",
    "    \n",
    "    for leaf in leaves:\n",
    "        max_lambda = raw_tree['lambda_val'][raw_tree['parent'] == leaf].max()\n",
    "        points = raw_tree['child'][(raw_tree['parent'] == leaf) & (raw_tree['lambda_val'] == max_lambda)]\n",
    "        result = np.hstack((result, points))\n",
    "        \n",
    "    return result.astype(np.int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cfb9187",
   "metadata": {},
   "outputs": [],
   "source": [
    "clusterer = topic_model.hdbscan_model\n",
    "tree = clusterer.condensed_tree_\n",
    "clusters = tree._select_clusters()\n",
    "len(clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8856856",
   "metadata": {},
   "outputs": [],
   "source": [
    "import hdbscan\n",
    "# Get the clusterer model, the clusters' tree and the clusters (topics ids)\n",
    "clusterer = topic_model.hdbscan_model\n",
    "tree = clusterer.condensed_tree_\n",
    "clusters = tree._select_clusters()\n",
    "\n",
    "# Get the ids of the most relevant documents (exemplars) associated with the topic at index idx\n",
    "c_exemplars = get_most_relevant_documents(clusters[1], tree)\n",
    "c_exemplars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ca02d34",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(c_exemplars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8621919b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(topic_model.topic_names[1])\n",
    "docs[9458]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "332df123",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_id = clusters[1]\n",
    "condensed_tree = tree\n",
    "\n",
    "\n",
    "raw_tree = condensed_tree._raw_tree\n",
    "\n",
    "# Just the cluster elements of the tree, excluding singleton points\n",
    "cluster_tree = raw_tree[raw_tree['child_size'] > 1]\n",
    "\n",
    "# Get the leaf cluster nodes under the cluster we are considering\n",
    "leaves = hdbscan.plots._recurse_leaf_dfs(cluster_tree, cluster_id)\n",
    "print(leaves)\n",
    "\n",
    "# Now collect up the last remaining points of each leaf cluster (the heart of the leaf)\n",
    "result = np.array([])\n",
    "\n",
    "\n",
    "for leaf in leaves:\n",
    "    max_lambda = raw_tree['lambda_val'][raw_tree['parent'] == leaf].max()\n",
    "    print(f'leaf = {leaf} max_lambda = {max_lambda}')\n",
    "    points = raw_tree[['child', 'lambda_val']][(raw_tree['parent'] == leaf) & (raw_tree['lambda_val'] == max_lambda)]\n",
    "    #points = raw_tree[['child', 'lambda_val']][(raw_tree['parent'] == leaf)]\n",
    "    #print(points)\n",
    "    #result = np.hstack((result, points))\n",
    "\n",
    "#raw_tree[['parent', 'child', 'lambda_val']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d170662",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install networkx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78e92028",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install networkx\n",
    "import networkx\n",
    "clusterer = topic_model.hdbscan_model\n",
    "tree = clusterer.condensed_tree_\n",
    "clusters = tree._select_clusters()\n",
    "\n",
    "xx = clusterer.condensed_tree_.to_networkx()\n",
    "#type(clusterer)\n",
    "xx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58019e9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#xx.number_of_nodes()\n",
    "plt.figure(figsize=(12,12))\n",
    "edges = xx.edges()\n",
    "pos = networkx.spring_layout(xx, k = 0.5) # k regulates the distance between nodes\n",
    "weights = [xx[u][v]['weight'] for u,v in edges]\n",
    "networkx.draw(xx, with_labels=True, node_color='skyblue', font_weight='bold',  width=weights, pos=pos)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98bfa681",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb729084",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "314.396px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
