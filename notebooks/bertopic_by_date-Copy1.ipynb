{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c8691738",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "34710da9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from LeWagon_FinalProject.data import DataProcessor\n",
    "from bertopic import BERTopic\n",
    "import hdbscan\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6237b63",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "a8174f6b",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def generate_docs(df_, number_of_docs):\n",
    "    df_ = df_[['date', 'content']][0:number_of_docs].copy().reset_index(drop=True)\n",
    "    df_.to_csv(f'../raw_data/BERTDocsContent_{str(number_of_docs)}.csv', header=True, index=False, encoding='utf-8')\n",
    "    return df_.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "1221ad9c",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def generate_topic_info(bert_model, number_of_docs):\n",
    "    df_topic_info = bert_model.get_topic_info()\n",
    "\n",
    "    df_topic_info.to_csv(f'../raw_data/BERTopicInfo_{str(number_of_docs)}.csv', header=True, index=False, encoding='utf-8')\n",
    "    return df_topic_info.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "734f46b8",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def generate_terms(bert_model, number_of_docs):\n",
    "    topics = bert_model.get_topics()\n",
    "    number_of_topics = len(topics)-1\n",
    "    \n",
    "    topic_columns = ['topic', 'term', 'weight']\n",
    "\n",
    "    df_topics = pd.DataFrame(columns=topic_columns)\n",
    "    for i in range(-1,number_of_topics): \n",
    "        num_of_terms = len(topics[i])\n",
    "        for j in range(num_of_terms):\n",
    "            new_topic = {}\n",
    "            new_topic['topic'] = topic_model.topic_names[i]\n",
    "            new_topic['term'] = topics[i][j][0]\n",
    "            new_topic['weight'] = round(topics[i][j][1],6)\n",
    "            df_topics = df_topics.append(new_topic, ignore_index=True)\n",
    "\n",
    "    df_topics.to_csv(f'../raw_data/BERTopicTerms_{str(number_of_docs)}.csv', header=True, index=False, encoding='utf-8')\n",
    "    return df_topics.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "ff2635dd",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def correlation_matrix_to_df(df_corr):\n",
    "    list_done = []\n",
    "    lits_item1 = []\n",
    "    lits_item2 = []\n",
    "    list_corr = []\n",
    "\n",
    "    for k in range(1,df_corr.shape[1]):\n",
    "        for i, j in df_corr.iterrows():\n",
    "            #if (df_corr.columns[k] != j[0]) and (j[0] not in list_done):\n",
    "            #if (j[0] not in list_done):\n",
    "            lits_item1.append(df_corr.columns[k])\n",
    "            lits_item2.append(j[0])\n",
    "            list_corr.append(j[k])\n",
    "        list_done.append(df_corr.columns[k])\n",
    "\n",
    "    corr_dict = {'topic1': lits_item1,\n",
    "                 'topic2': lits_item2,\n",
    "                 'similarity': list_corr}\n",
    "    df_res = pd.DataFrame(corr_dict)\n",
    "    df_res = df_res.sort_values(by='similarity', ascending=False).copy()\n",
    "    df_res.reset_index(inplace=True,drop=True)\n",
    "    return df_res.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "2304d11b",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def generate_topic_similarity(bert_model, number_of_docs):\n",
    "    corr_matrix = bert_model.topic_sim_matrix\n",
    "\n",
    "    topics = bert_model.get_topics()\n",
    "    number_of_topics = len(topics)-1\n",
    "\n",
    "    topic_columns = ['topic']\n",
    "    for i in range(-1,number_of_topics):\n",
    "        topic_columns.append(bert_model.topic_names[i])\n",
    "\n",
    "    df_similarity = pd.DataFrame(columns=topic_columns)\n",
    "    for i in range(-1,number_of_topics):\n",
    "        new_topic = {}\n",
    "        new_topic['topic'] = bert_model.topic_names[i]\n",
    "        for j in range(-1,number_of_topics):\n",
    "            new_topic[bert_model.topic_names[j]] = round(corr_matrix[i,j],6)\n",
    "        df_similarity = df_similarity.append(new_topic, ignore_index=True)\n",
    "        \n",
    "    df_topic_similarity = correlation_matrix_to_df(df_similarity)\n",
    "    df_topic_similarity.to_csv(f'../raw_data/BERTopicSimilarity_{str(number_of_docs)}.csv', header=True, index=False, encoding='utf-8')\n",
    "    return df_topic_similarity.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "70084133",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def get_topic_documents(cluster_id, condensed_tree):\n",
    "    result_points = np.array([])\n",
    "    result_points_val = np.array([])\n",
    "    \n",
    "    #assert cluster_id > -1, \"The topic's label should be greater than -1!\"\n",
    "    \n",
    "    if cluster_id <= -1:\n",
    "        return result_points.astype(np.int64), result_points_val.astype(np.float64)\n",
    "        \n",
    "    raw_tree = condensed_tree._raw_tree\n",
    "    \n",
    "    # Just the cluster elements of the tree, excluding singleton points\n",
    "    cluster_tree = raw_tree[raw_tree['child_size'] > 1]\n",
    "    \n",
    "    # Get the leaf cluster nodes under the cluster we are considering\n",
    "    leaves = hdbscan.plots._recurse_leaf_dfs(cluster_tree, cluster_id)\n",
    "    \n",
    "    # Now collect up the last remaining points of each leaf cluster (the heart of the leaf) \n",
    "    for leaf in leaves:\n",
    "        #max_lambda = raw_tree['lambda_val'][raw_tree['parent'] == leaf].max()\n",
    "        #points = raw_tree['child'][(raw_tree['parent'] == leaf) & (raw_tree['lambda_val'] == max_lambda)]\n",
    "        #points_val = raw_tree['lambda_val'][(raw_tree['parent'] == leaf) & (raw_tree['lambda_val'] == max_lambda)]\n",
    "        points = raw_tree['child'][(raw_tree['parent'] == leaf)]\n",
    "        points_val = raw_tree['lambda_val'][(raw_tree['parent'] == leaf)]\n",
    "        result_points = np.hstack((result_points, points))\n",
    "        result_points_val = np.hstack((result_points_val, points_val))\n",
    "        \n",
    "    return result_points.astype(np.int64), result_points_val.astype(np.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "9cf25505",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def generate_topic_documents(bert_model, number_of_docs):\n",
    "    clusterer = bert_model.hdbscan_model\n",
    "    tree = clusterer.condensed_tree_\n",
    "    clusters = tree._select_clusters()\n",
    "\n",
    "    number_of_topics = len(clusters)\n",
    "\n",
    "    relevant_columns = ['topic', 'document', 'lambda_val']\n",
    "    df_rel_docs = pd.DataFrame(columns=relevant_columns)\n",
    "        \n",
    "    if number_of_topics == len(bert_model.get_topics()):\n",
    "        start_ind = -1\n",
    "    else:\n",
    "        start_ind = 0\n",
    "\n",
    "    for i in range(0, number_of_topics):\n",
    "        rel_docs, lambda_vals = get_topic_documents(clusters[i], tree)\n",
    "        if len(rel_docs) > 0:\n",
    "            if start_ind < 0:\n",
    "                topic_name = bert_model.topic_names[i-1]\n",
    "            else:\n",
    "                topic_name = bert_model.topic_names[i]\n",
    "                \n",
    "            for j in range(0, len(rel_docs)):\n",
    "                new_doc_rel = {}\n",
    "                new_doc_rel['topic'] = topic_name\n",
    "                new_doc_rel['document'] = rel_docs[j]\n",
    "                new_doc_rel['lambda_val'] = round(lambda_vals[j],6)\n",
    "                df_rel_docs = df_rel_docs.append(new_doc_rel, ignore_index=True)\n",
    "\n",
    "    df_rel_docs.to_csv(f'../raw_data/BERTopicDocuments_{str(number_of_docs)}.csv', header=True, index=False, encoding='utf-8')\n",
    "    return df_rel_docs.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "3def10c2",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def get_cosine_similarity(feature_vec_1, feature_vec_2):\n",
    "    return cosine_similarity(feature_vec_1.reshape(1, -1), feature_vec_2.reshape(1, -1))[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "8a260c4d",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n    sim_columns = ['cosine_similarity', 'document1', 'document2']\\n    df_sim_docs = pd.DataFrame(columns=sim_columns)\\n    temp_columns = ['cosine_similarity']\\n    for i in range(0, len(docs)):\\n        docs_sim = df_documents_similarity[i]\\n        df_sim_docs_temp = pd.DataFrame(data = docs_sim, columns=temp_columns)\\n        df_sim_docs_temp['document1'] = i\\n        df_sim_docs_temp['document2'] = df_sim_docs_temp.index\\n        df_sim_docs = df_sim_docs.append(df_sim_docs_temp, ignore_index=True)\\n    \\n    #df_sim_docs.to_csv(f'../raw_data/BERTopicDocumentsSimilarity_{str(number_of_docs)}.csv', header=True, index=False, encoding='utf-8')     \\n    return df_sim_docs.copy()\\n\""
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def generate_documents_similarity(bert_model, docs, number_of_docs):\n",
    "    emb_model = bert_model.embedding_model\n",
    "    \n",
    "    # Create documents embeddings\n",
    "    embeddings = emb_model.embedding_model.encode(docs)\n",
    "    doc_sim_matrix = cosine_similarity(embeddings, embeddings)\n",
    "    np.savetxt(f'../raw_data/BERTopicDocumentsSimilarity_{str(number_of_docs)}.csv', doc_sim_matrix, delimiter=',')\n",
    "    np.save(f'../raw_data/BERTopicDocumentsSimilarity_{str(number_of_docs)}.npy', doc_sim_matrix)\n",
    "    return doc_sim_matrix\n",
    "'''\n",
    "    sim_columns = ['cosine_similarity', 'document1', 'document2']\n",
    "    df_sim_docs = pd.DataFrame(columns=sim_columns)\n",
    "    temp_columns = ['cosine_similarity']\n",
    "    for i in range(0, len(docs)):\n",
    "        docs_sim = df_documents_similarity[i]\n",
    "        df_sim_docs_temp = pd.DataFrame(data = docs_sim, columns=temp_columns)\n",
    "        df_sim_docs_temp['document1'] = i\n",
    "        df_sim_docs_temp['document2'] = df_sim_docs_temp.index\n",
    "        df_sim_docs = df_sim_docs.append(df_sim_docs_temp, ignore_index=True)\n",
    "    \n",
    "    #df_sim_docs.to_csv(f'../raw_data/BERTopicDocumentsSimilarity_{str(number_of_docs)}.csv', header=True, index=False, encoding='utf-8')     \n",
    "    return df_sim_docs.copy()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "b1d13d1e",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def get_topic_start_end_dates(mode_index, topic_index):\n",
    "    df_topic = pd.read_csv(f'../raw_data/BERTopicInfo_{str(mode_index)}.csv')\n",
    "    topic_name = df_topic[df_topic['Topic']==topic_index]['Name'].values[0]\n",
    "\n",
    "    df_documents = pd.read_csv(f'../raw_data/BERTopicDocuments_{str(mode_index)}.csv')\n",
    "    df_documents = df_documents[df_documents['topic']==topic_name]\n",
    "\n",
    "    df_docscontent = pd.read_csv(f'../raw_data/BERTopicDocsContent_{str(mode_index)}.csv', parse_dates=True)\n",
    "    df_docscontent = df_docscontent[df_docscontent.index.isin(df_documents['document'].values)]\n",
    "    start_date = df_docscontent['date'].min()\n",
    "    end_date = df_docscontent['date'].max()\n",
    "    number_topic_docs = len(df_documents)\n",
    "    return start_date, end_date, number_topic_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19af20e7",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Generate data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a641eb31",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "'''dp1 = DataProcessor(csv_path='../raw_data/', csv_name='articles1')\n",
    "df1 = dp1.load_dataset()\n",
    "\n",
    "dp2 = DataProcessor(csv_path='../raw_data/', csv_name='articles2')\n",
    "df2 = dp2.load_dataset()\n",
    "\n",
    "dp3 = DataProcessor(csv_path='../raw_data/', csv_name='articles3')\n",
    "df3 = dp3.load_dataset()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94a97c90",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "'''df_all = df1.copy()\n",
    "df_all = df_all.append(df2, ignore_index=True)\n",
    "df_all = df_all.append(df3, ignore_index=True)\n",
    "df_all = df_all.sort_values(by=['year', 'month'], ascending=True).reset_index(drop=True)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a87c8560",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "'''print(df_all.shape)\n",
    "#df_all[(df_all['year'].isna())].dropna(inplace=True)\n",
    "df_all.dropna(subset=['year', 'month'], inplace=True)\n",
    "print(df_all.shape)\n",
    "df_all\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dc77873",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "'''df_all = df_all[['id', 'title', 'year', 'month', 'content']].copy()\n",
    "df_all = df_all[df_all['year'] >= 2015].copy()\n",
    "df_all\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06a67414",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#df_all.to_csv(f'../raw_data/dataset_work.csv', header=True, index=True, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bee80cd4",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "'''import requests\n",
    "url = 'https://bucketapipython-guadc7haza-uc.a.run.app/data'\n",
    "\n",
    "params = {'filename': 'dataset_work.csv', 'data': df_all.to_json()}\n",
    "\n",
    "x = requests.post(url, params=params)\n",
    "print(x.text)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ba9f379",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "'''import requests\n",
    "url = 'https://bucketapipython-guadc7haza-uc.a.run.app/data'\n",
    "params = {'filename': 'dataset_work', 'extension': 'csv'}\n",
    "x = requests.get(url, params=params)\n",
    "x'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "58fbbe35",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "number_of_docs = 3_000\n",
    "\n",
    "dp = DataProcessor(csv_path='../raw_data/', csv_name='political_dataset')\n",
    "df = dp.load_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "b3d71165",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>415</td>\n",
       "      <td>36361</td>\n",
       "      <td>2015: Sold Out South Carolina Tea Party Conven...</td>\n",
       "      <td>2015.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>MYRTLE BEACH, South Carolina  —   The South Ca...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>417</td>\n",
       "      <td>57593</td>\n",
       "      <td>Narendra Modi Fast Facts</td>\n",
       "      <td>2015.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>(CNN) Here is a look at the life of India’s P...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>418</td>\n",
       "      <td>59225</td>\n",
       "      <td>Little Richard Fast Facts</td>\n",
       "      <td>2015.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>(CNN) Here is a look at the life of   ”Archit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>420</td>\n",
       "      <td>60219</td>\n",
       "      <td>Cycling’s marathon man attempts 75,000 miles i...</td>\n",
       "      <td>2015.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>(CNN) While many people are recovering from a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>422</td>\n",
       "      <td>60223</td>\n",
       "      <td>Cops: Georgia police chief on leave after wife...</td>\n",
       "      <td>2015.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>(CNN) Magazines and websites regularly rank P...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0     id                                              title  \\\n",
       "0         415  36361  2015: Sold Out South Carolina Tea Party Conven...   \n",
       "1         417  57593                           Narendra Modi Fast Facts   \n",
       "2         418  59225                          Little Richard Fast Facts   \n",
       "3         420  60219  Cycling’s marathon man attempts 75,000 miles i...   \n",
       "4         422  60223  Cops: Georgia police chief on leave after wife...   \n",
       "\n",
       "     year  month                                            content  \n",
       "0  2015.0    1.0  MYRTLE BEACH, South Carolina  —   The South Ca...  \n",
       "1  2015.0    1.0   (CNN) Here is a look at the life of India’s P...  \n",
       "2  2015.0    1.0   (CNN) Here is a look at the life of   ”Archit...  \n",
       "3  2015.0    1.0   (CNN) While many people are recovering from a...  \n",
       "4  2015.0    1.0   (CNN) Magazines and websites regularly rank P...  "
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "03d805b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = df['content'][2000:number_of_docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "f9bf545a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords \n",
    "stop_words = stopwords.words('english')\n",
    "#stop_words += stopwords.words('portuguese')\n",
    "stop_words = set(stop_words)\n",
    "from nltk import word_tokenize\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    tokenized = word_tokenize(text)\n",
    "    without_stopwords = [word for word in tokenized if not word in stop_words]\n",
    "    return ' '.join(without_stopwords)\n",
    "\n",
    "docs = docs.apply(lambda x: remove_stopwords(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "46e4ed34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2000    It looks like Kentucky Gov . Matt Bevin ( R ) ...\n",
       "2001    TPM pleased announce winners Ninth Annual Gold...\n",
       "2002    Going back post yesterday US murder rate , I w...\n",
       "2003    As I ’ mentioned number posts years , I believ...\n",
       "2004    Pundits seem problem getting heads around . Do...\n",
       "Name: content, dtype: object"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "411e8830",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([-1,\n",
       "  -1,\n",
       "  1,\n",
       "  1,\n",
       "  3,\n",
       "  10,\n",
       "  0,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  9,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  6,\n",
       "  -1,\n",
       "  -1,\n",
       "  8,\n",
       "  8,\n",
       "  1,\n",
       "  9,\n",
       "  -1,\n",
       "  -1,\n",
       "  8,\n",
       "  8,\n",
       "  3,\n",
       "  3,\n",
       "  1,\n",
       "  6,\n",
       "  -1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  9,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  3,\n",
       "  7,\n",
       "  -1,\n",
       "  7,\n",
       "  -1,\n",
       "  8,\n",
       "  -1,\n",
       "  1,\n",
       "  -1,\n",
       "  1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  1,\n",
       "  1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  1,\n",
       "  9,\n",
       "  1,\n",
       "  -1,\n",
       "  0,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  0,\n",
       "  -1,\n",
       "  -1,\n",
       "  1,\n",
       "  -1,\n",
       "  0,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  9,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  0,\n",
       "  9,\n",
       "  -1,\n",
       "  1,\n",
       "  -1,\n",
       "  10,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  8,\n",
       "  -1,\n",
       "  -1,\n",
       "  6,\n",
       "  6,\n",
       "  -1,\n",
       "  10,\n",
       "  -1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  -1,\n",
       "  0,\n",
       "  3,\n",
       "  -1,\n",
       "  8,\n",
       "  -1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  9,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  1,\n",
       "  -1,\n",
       "  -1,\n",
       "  1,\n",
       "  8,\n",
       "  5,\n",
       "  -1,\n",
       "  -1,\n",
       "  1,\n",
       "  -1,\n",
       "  6,\n",
       "  10,\n",
       "  11,\n",
       "  -1,\n",
       "  6,\n",
       "  5,\n",
       "  -1,\n",
       "  1,\n",
       "  -1,\n",
       "  0,\n",
       "  4,\n",
       "  6,\n",
       "  9,\n",
       "  0,\n",
       "  -1,\n",
       "  0,\n",
       "  -1,\n",
       "  5,\n",
       "  5,\n",
       "  0,\n",
       "  7,\n",
       "  6,\n",
       "  -1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  5,\n",
       "  0,\n",
       "  0,\n",
       "  2,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  0,\n",
       "  -1,\n",
       "  4,\n",
       "  7,\n",
       "  0,\n",
       "  -1,\n",
       "  1,\n",
       "  3,\n",
       "  0,\n",
       "  -1,\n",
       "  5,\n",
       "  -1,\n",
       "  4,\n",
       "  9,\n",
       "  -1,\n",
       "  10,\n",
       "  10,\n",
       "  0,\n",
       "  8,\n",
       "  7,\n",
       "  -1,\n",
       "  -1,\n",
       "  0,\n",
       "  2,\n",
       "  0,\n",
       "  -1,\n",
       "  -1,\n",
       "  1,\n",
       "  3,\n",
       "  3,\n",
       "  4,\n",
       "  1,\n",
       "  9,\n",
       "  -1,\n",
       "  12,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  5,\n",
       "  -1,\n",
       "  6,\n",
       "  -1,\n",
       "  4,\n",
       "  0,\n",
       "  6,\n",
       "  -1,\n",
       "  3,\n",
       "  2,\n",
       "  -1,\n",
       "  4,\n",
       "  7,\n",
       "  0,\n",
       "  -1,\n",
       "  3,\n",
       "  -1,\n",
       "  -1,\n",
       "  0,\n",
       "  0,\n",
       "  -1,\n",
       "  7,\n",
       "  2,\n",
       "  -1,\n",
       "  -1,\n",
       "  10,\n",
       "  5,\n",
       "  -1,\n",
       "  -1,\n",
       "  4,\n",
       "  2,\n",
       "  -1,\n",
       "  6,\n",
       "  6,\n",
       "  3,\n",
       "  1,\n",
       "  3,\n",
       "  12,\n",
       "  -1,\n",
       "  -1,\n",
       "  7,\n",
       "  -1,\n",
       "  -1,\n",
       "  9,\n",
       "  2,\n",
       "  4,\n",
       "  0,\n",
       "  7,\n",
       "  -1,\n",
       "  4,\n",
       "  8,\n",
       "  -1,\n",
       "  2,\n",
       "  0,\n",
       "  0,\n",
       "  11,\n",
       "  0,\n",
       "  -1,\n",
       "  5,\n",
       "  -1,\n",
       "  7,\n",
       "  9,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  1,\n",
       "  -1,\n",
       "  -1,\n",
       "  1,\n",
       "  8,\n",
       "  2,\n",
       "  8,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  10,\n",
       "  0,\n",
       "  -1,\n",
       "  10,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  10,\n",
       "  -1,\n",
       "  7,\n",
       "  6,\n",
       "  8,\n",
       "  3,\n",
       "  10,\n",
       "  6,\n",
       "  0,\n",
       "  2,\n",
       "  4,\n",
       "  -1,\n",
       "  -1,\n",
       "  2,\n",
       "  -1,\n",
       "  -1,\n",
       "  1,\n",
       "  0,\n",
       "  9,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  2,\n",
       "  5,\n",
       "  -1,\n",
       "  0,\n",
       "  12,\n",
       "  2,\n",
       "  0,\n",
       "  11,\n",
       "  4,\n",
       "  0,\n",
       "  10,\n",
       "  5,\n",
       "  0,\n",
       "  2,\n",
       "  -1,\n",
       "  7,\n",
       "  -1,\n",
       "  3,\n",
       "  2,\n",
       "  4,\n",
       "  9,\n",
       "  -1,\n",
       "  4,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  0,\n",
       "  4,\n",
       "  4,\n",
       "  2,\n",
       "  -1,\n",
       "  -1,\n",
       "  5,\n",
       "  9,\n",
       "  -1,\n",
       "  9,\n",
       "  6,\n",
       "  7,\n",
       "  12,\n",
       "  0,\n",
       "  1,\n",
       "  11,\n",
       "  4,\n",
       "  1,\n",
       "  -1,\n",
       "  -1,\n",
       "  2,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  6,\n",
       "  3,\n",
       "  -1,\n",
       "  6,\n",
       "  6,\n",
       "  1,\n",
       "  9,\n",
       "  -1,\n",
       "  -1,\n",
       "  12,\n",
       "  12,\n",
       "  -1,\n",
       "  -1,\n",
       "  6,\n",
       "  5,\n",
       "  -1,\n",
       "  -1,\n",
       "  0,\n",
       "  -1,\n",
       "  0,\n",
       "  5,\n",
       "  3,\n",
       "  3,\n",
       "  6,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  12,\n",
       "  8,\n",
       "  8,\n",
       "  -1,\n",
       "  9,\n",
       "  12,\n",
       "  7,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  7,\n",
       "  2,\n",
       "  9,\n",
       "  -1,\n",
       "  12,\n",
       "  3,\n",
       "  3,\n",
       "  2,\n",
       "  -1,\n",
       "  0,\n",
       "  8,\n",
       "  0,\n",
       "  3,\n",
       "  -1,\n",
       "  0,\n",
       "  6,\n",
       "  4,\n",
       "  11,\n",
       "  11,\n",
       "  0,\n",
       "  3,\n",
       "  -1,\n",
       "  7,\n",
       "  7,\n",
       "  3,\n",
       "  5,\n",
       "  0,\n",
       "  -1,\n",
       "  0,\n",
       "  3,\n",
       "  11,\n",
       "  9,\n",
       "  -1,\n",
       "  -1,\n",
       "  2,\n",
       "  9,\n",
       "  3,\n",
       "  1,\n",
       "  -1,\n",
       "  -1,\n",
       "  8,\n",
       "  7,\n",
       "  -1,\n",
       "  4,\n",
       "  1,\n",
       "  4,\n",
       "  -1,\n",
       "  10,\n",
       "  3,\n",
       "  3,\n",
       "  -1,\n",
       "  -1,\n",
       "  7,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  4,\n",
       "  -1,\n",
       "  4,\n",
       "  4,\n",
       "  0,\n",
       "  0,\n",
       "  5,\n",
       "  -1,\n",
       "  -1,\n",
       "  5,\n",
       "  5,\n",
       "  6,\n",
       "  4,\n",
       "  8,\n",
       "  8,\n",
       "  1,\n",
       "  -1,\n",
       "  5,\n",
       "  -1,\n",
       "  6,\n",
       "  2,\n",
       "  2,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  12,\n",
       "  8,\n",
       "  5,\n",
       "  11,\n",
       "  -1,\n",
       "  -1,\n",
       "  7,\n",
       "  8,\n",
       "  -1,\n",
       "  2,\n",
       "  0,\n",
       "  5,\n",
       "  -1,\n",
       "  7,\n",
       "  -1,\n",
       "  8,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  7,\n",
       "  -1,\n",
       "  -1,\n",
       "  3,\n",
       "  4,\n",
       "  11,\n",
       "  5,\n",
       "  -1,\n",
       "  1,\n",
       "  3,\n",
       "  -1,\n",
       "  0,\n",
       "  5,\n",
       "  11,\n",
       "  3,\n",
       "  0,\n",
       "  5,\n",
       "  8,\n",
       "  8,\n",
       "  -1,\n",
       "  4,\n",
       "  6,\n",
       "  1,\n",
       "  11,\n",
       "  4,\n",
       "  3,\n",
       "  -1,\n",
       "  -1,\n",
       "  11,\n",
       "  3,\n",
       "  12,\n",
       "  0,\n",
       "  2,\n",
       "  5,\n",
       "  12,\n",
       "  -1,\n",
       "  -1,\n",
       "  1,\n",
       "  8,\n",
       "  5,\n",
       "  -1,\n",
       "  7,\n",
       "  4,\n",
       "  11,\n",
       "  11,\n",
       "  5,\n",
       "  -1,\n",
       "  4,\n",
       "  0,\n",
       "  0,\n",
       "  8,\n",
       "  -1,\n",
       "  5,\n",
       "  -1,\n",
       "  0,\n",
       "  -1,\n",
       "  -1,\n",
       "  2,\n",
       "  -1,\n",
       "  -1,\n",
       "  0,\n",
       "  -1,\n",
       "  3,\n",
       "  5,\n",
       "  4,\n",
       "  -1,\n",
       "  0,\n",
       "  7,\n",
       "  2,\n",
       "  9,\n",
       "  6,\n",
       "  -1,\n",
       "  2,\n",
       "  3,\n",
       "  2,\n",
       "  0,\n",
       "  -1,\n",
       "  0,\n",
       "  6,\n",
       "  3,\n",
       "  7,\n",
       "  7,\n",
       "  -1,\n",
       "  4,\n",
       "  1,\n",
       "  2,\n",
       "  5,\n",
       "  -1,\n",
       "  0,\n",
       "  -1,\n",
       "  -1,\n",
       "  4,\n",
       "  3,\n",
       "  8,\n",
       "  -1,\n",
       "  3,\n",
       "  -1,\n",
       "  -1,\n",
       "  1,\n",
       "  9,\n",
       "  6,\n",
       "  6,\n",
       "  0,\n",
       "  5,\n",
       "  5,\n",
       "  8,\n",
       "  -1,\n",
       "  3,\n",
       "  5,\n",
       "  3,\n",
       "  -1,\n",
       "  10,\n",
       "  9,\n",
       "  7,\n",
       "  -1,\n",
       "  -1,\n",
       "  10,\n",
       "  -1,\n",
       "  9,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  5,\n",
       "  1,\n",
       "  -1,\n",
       "  1,\n",
       "  7,\n",
       "  2,\n",
       "  4,\n",
       "  3,\n",
       "  4,\n",
       "  0,\n",
       "  3,\n",
       "  -1,\n",
       "  -1,\n",
       "  1,\n",
       "  4,\n",
       "  -1,\n",
       "  3,\n",
       "  5,\n",
       "  12,\n",
       "  6,\n",
       "  4,\n",
       "  9,\n",
       "  10,\n",
       "  11,\n",
       "  -1,\n",
       "  4,\n",
       "  -1,\n",
       "  7,\n",
       "  11,\n",
       "  2,\n",
       "  4,\n",
       "  12,\n",
       "  0,\n",
       "  4,\n",
       "  0,\n",
       "  0,\n",
       "  2,\n",
       "  -1,\n",
       "  0,\n",
       "  1,\n",
       "  -1,\n",
       "  2,\n",
       "  12,\n",
       "  8,\n",
       "  0,\n",
       "  -1,\n",
       "  -1,\n",
       "  1,\n",
       "  -1,\n",
       "  7,\n",
       "  4,\n",
       "  -1,\n",
       "  6,\n",
       "  -1,\n",
       "  7,\n",
       "  -1,\n",
       "  -1,\n",
       "  6,\n",
       "  11,\n",
       "  5,\n",
       "  0,\n",
       "  5,\n",
       "  1,\n",
       "  1,\n",
       "  5,\n",
       "  0,\n",
       "  2,\n",
       "  9,\n",
       "  8,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  7,\n",
       "  -1,\n",
       "  0,\n",
       "  4,\n",
       "  2,\n",
       "  2,\n",
       "  0,\n",
       "  0,\n",
       "  4,\n",
       "  2,\n",
       "  12,\n",
       "  10,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  4,\n",
       "  -1,\n",
       "  5,\n",
       "  2,\n",
       "  1,\n",
       "  4,\n",
       "  -1,\n",
       "  4,\n",
       "  0,\n",
       "  -1,\n",
       "  1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  0,\n",
       "  -1,\n",
       "  2,\n",
       "  4,\n",
       "  -1,\n",
       "  8,\n",
       "  -1,\n",
       "  0,\n",
       "  10,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  0,\n",
       "  5,\n",
       "  2,\n",
       "  -1,\n",
       "  -1,\n",
       "  12,\n",
       "  -1,\n",
       "  12,\n",
       "  -1,\n",
       "  1,\n",
       "  7,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  -1,\n",
       "  -1,\n",
       "  0,\n",
       "  -1,\n",
       "  1,\n",
       "  5,\n",
       "  2,\n",
       "  -1,\n",
       "  7,\n",
       "  3,\n",
       "  8,\n",
       "  0,\n",
       "  3,\n",
       "  1,\n",
       "  12,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  3,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  3,\n",
       "  1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  1,\n",
       "  2,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  2,\n",
       "  -1,\n",
       "  3,\n",
       "  -1,\n",
       "  6,\n",
       "  0,\n",
       "  9,\n",
       "  -1,\n",
       "  -1,\n",
       "  2,\n",
       "  6,\n",
       "  8,\n",
       "  11,\n",
       "  -1,\n",
       "  -1,\n",
       "  6,\n",
       "  2,\n",
       "  0,\n",
       "  3,\n",
       "  0,\n",
       "  1,\n",
       "  4,\n",
       "  0,\n",
       "  -1,\n",
       "  -1,\n",
       "  2,\n",
       "  4,\n",
       "  -1,\n",
       "  -1,\n",
       "  10,\n",
       "  12,\n",
       "  4,\n",
       "  -1,\n",
       "  0,\n",
       "  -1,\n",
       "  5,\n",
       "  6,\n",
       "  -1,\n",
       "  11,\n",
       "  5,\n",
       "  2,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  8,\n",
       "  -1,\n",
       "  0,\n",
       "  2,\n",
       "  6,\n",
       "  0,\n",
       "  2,\n",
       "  -1,\n",
       "  -1,\n",
       "  0,\n",
       "  -1,\n",
       "  -1,\n",
       "  0,\n",
       "  3,\n",
       "  1,\n",
       "  0,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  0,\n",
       "  -1,\n",
       "  0,\n",
       "  3,\n",
       "  0,\n",
       "  11,\n",
       "  -1,\n",
       "  -1,\n",
       "  11,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  1,\n",
       "  0,\n",
       "  6,\n",
       "  4,\n",
       "  -1,\n",
       "  -1,\n",
       "  5,\n",
       "  -1,\n",
       "  12,\n",
       "  0,\n",
       "  6,\n",
       "  -1,\n",
       "  6,\n",
       "  2,\n",
       "  -1,\n",
       "  7,\n",
       "  8,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  3,\n",
       "  1,\n",
       "  -1,\n",
       "  -1,\n",
       "  1,\n",
       "  10,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  0,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  4,\n",
       "  5,\n",
       "  0,\n",
       "  10,\n",
       "  3,\n",
       "  5,\n",
       "  0,\n",
       "  0,\n",
       "  -1,\n",
       "  3,\n",
       "  0,\n",
       "  11,\n",
       "  2,\n",
       "  2,\n",
       "  -1,\n",
       "  -1,\n",
       "  5,\n",
       "  -1,\n",
       "  3,\n",
       "  -1,\n",
       "  9,\n",
       "  7,\n",
       "  -1,\n",
       "  4,\n",
       "  -1,\n",
       "  8,\n",
       "  8,\n",
       "  0,\n",
       "  2,\n",
       "  -1,\n",
       "  2,\n",
       "  0,\n",
       "  -1,\n",
       "  7,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  10,\n",
       "  10,\n",
       "  -1,\n",
       "  1,\n",
       "  1,\n",
       "  3,\n",
       "  8,\n",
       "  -1,\n",
       "  5,\n",
       "  -1,\n",
       "  -1,\n",
       "  9,\n",
       "  6,\n",
       "  -1,\n",
       "  9,\n",
       "  0,\n",
       "  0,\n",
       "  2,\n",
       "  1,\n",
       "  4,\n",
       "  6,\n",
       "  1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  7,\n",
       "  5,\n",
       "  -1,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  12,\n",
       "  -1,\n",
       "  11,\n",
       "  0,\n",
       "  0,\n",
       "  3,\n",
       "  1,\n",
       "  10,\n",
       "  -1,\n",
       "  0,\n",
       "  -1,\n",
       "  0,\n",
       "  7,\n",
       "  6,\n",
       "  2,\n",
       "  2,\n",
       "  4],\n",
       " None)"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# sentence_model = SentenceTransformer(\"paraphrase-MiniLM-L6-v2\")\n",
    "sentence_model = SentenceTransformer(\"paraphrase-mpnet-base-v2\")\n",
    "\n",
    "topic_model = BERTopic(embedding_model=sentence_model, min_topic_size=20, language='english', calculate_probabilities=False, n_gram_range=(2,2))\n",
    "topic_model.fit_transform(np.array(docs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "4877ecde",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Topic</th>\n",
       "      <th>Count</th>\n",
       "      <th>Name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1</td>\n",
       "      <td>378</td>\n",
       "      <td>-1_donald trump_united states_breitbart news_t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>112</td>\n",
       "      <td>0_islamic state_breitbart london_asylum seeker...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>76</td>\n",
       "      <td>1_gun control_second amendment_background chec...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>58</td>\n",
       "      <td>2_fox news_primary debate_news channel_gop fro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>55</td>\n",
       "      <td>3_bill clinton_hillary clinton_sexual assault_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>4</td>\n",
       "      <td>53</td>\n",
       "      <td>4_ted cruz_donald trump_sen ted_york values</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>5</td>\n",
       "      <td>49</td>\n",
       "      <td>5_academy awards_pinkett smith_jada pinkett_pe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>6</td>\n",
       "      <td>42</td>\n",
       "      <td>6_ted cruz_iowa caucus_trump cruz_among evange...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>7</td>\n",
       "      <td>39</td>\n",
       "      <td>7_bernie sanders_hillary clinton_sanders says_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>8</td>\n",
       "      <td>38</td>\n",
       "      <td>8_republican party_schlafly said_billionaires ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>9</td>\n",
       "      <td>31</td>\n",
       "      <td>9_hillary clinton_classified information_top s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>10</td>\n",
       "      <td>24</td>\n",
       "      <td>10_milo yiannopoulos_social media_twitter jess...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>11</td>\n",
       "      <td>23</td>\n",
       "      <td>11_state union_nuclear deal_president obama_ir...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>12</td>\n",
       "      <td>22</td>\n",
       "      <td>12_natural born_born citizen_ted cruz_cruz born</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Topic  Count                                               Name\n",
       "0      -1    378  -1_donald trump_united states_breitbart news_t...\n",
       "1       0    112  0_islamic state_breitbart london_asylum seeker...\n",
       "2       1     76  1_gun control_second amendment_background chec...\n",
       "3       2     58  2_fox news_primary debate_news channel_gop fro...\n",
       "4       3     55  3_bill clinton_hillary clinton_sexual assault_...\n",
       "5       4     53        4_ted cruz_donald trump_sen ted_york values\n",
       "6       5     49  5_academy awards_pinkett smith_jada pinkett_pe...\n",
       "7       6     42  6_ted cruz_iowa caucus_trump cruz_among evange...\n",
       "8       7     39  7_bernie sanders_hillary clinton_sanders says_...\n",
       "9       8     38  8_republican party_schlafly said_billionaires ...\n",
       "10      9     31  9_hillary clinton_classified information_top s...\n",
       "11     10     24  10_milo yiannopoulos_social media_twitter jess...\n",
       "12     11     23  11_state union_nuclear deal_president obama_ir...\n",
       "13     12     22    12_natural born_born citizen_ted cruz_cruz born"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_model.get_topic_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "918989f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Topic</th>\n",
       "      <th>Count</th>\n",
       "      <th>Name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1</td>\n",
       "      <td>309</td>\n",
       "      <td>-1_donald trump_ted cruz_marco rubio_republica...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>218</td>\n",
       "      <td>0_planned parenthood_breitbart news_zika virus...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>115</td>\n",
       "      <td>1_hillary clinton_bill clinton_bernie sanders_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>108</td>\n",
       "      <td>2_islamic state_new year eve_breitbart london_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>81</td>\n",
       "      <td>3_academy awards_pinkett smith_people color_ja...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>4</td>\n",
       "      <td>49</td>\n",
       "      <td>4_gun control_executive gun_awr hawkins twitte...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>5</td>\n",
       "      <td>43</td>\n",
       "      <td>5_breitbart news_milo yiannopoulos_january 201...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>6</td>\n",
       "      <td>30</td>\n",
       "      <td>6_hillary clinton_classified information_priva...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>7</td>\n",
       "      <td>24</td>\n",
       "      <td>7_state union_nuclear deal_january 13 2016_sta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>8</td>\n",
       "      <td>23</td>\n",
       "      <td>8_natural born_born citizen_natural born citiz...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Topic  Count                                               Name\n",
       "0     -1    309  -1_donald trump_ted cruz_marco rubio_republica...\n",
       "1      0    218  0_planned parenthood_breitbart news_zika virus...\n",
       "2      1    115  1_hillary clinton_bill clinton_bernie sanders_...\n",
       "3      2    108  2_islamic state_new year eve_breitbart london_...\n",
       "4      3     81  3_academy awards_pinkett smith_people color_ja...\n",
       "5      4     49  4_gun control_executive gun_awr hawkins twitte...\n",
       "6      5     43  5_breitbart news_milo yiannopoulos_january 201...\n",
       "7      6     30  6_hillary clinton_classified information_priva...\n",
       "8      7     24  7_state union_nuclear deal_january 13 2016_sta...\n",
       "9      8     23  8_natural born_born citizen_natural born citiz..."
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_model.get_topic_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "405b7358",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Topic</th>\n",
       "      <th>Count</th>\n",
       "      <th>Name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1</td>\n",
       "      <td>284</td>\n",
       "      <td>-1_ted cruz_marco rubio_republican presidentia...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>240</td>\n",
       "      <td>0_donald trump_united states_planned parenthoo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>118</td>\n",
       "      <td>1_islamic state_new year_year eve_new year eve</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>117</td>\n",
       "      <td>2_hillary clinton_bill clinton_bernie sanders_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>81</td>\n",
       "      <td>3_academy awards_pinkett smith_jada pinkett sm...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>4</td>\n",
       "      <td>49</td>\n",
       "      <td>4_gun control_awr hawkins_executive gun_follow...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>5</td>\n",
       "      <td>35</td>\n",
       "      <td>5_breitbart news_milo yiannopoulos_breitbart t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>6</td>\n",
       "      <td>30</td>\n",
       "      <td>6_hillary clinton_private server_secretary sta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>7</td>\n",
       "      <td>23</td>\n",
       "      <td>7_state union_nuclear deal_january 13 2016_uni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>8</td>\n",
       "      <td>23</td>\n",
       "      <td>8_natural born_born citizen_natural born citiz...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Topic  Count                                               Name\n",
       "0     -1    284  -1_ted cruz_marco rubio_republican presidentia...\n",
       "1      0    240  0_donald trump_united states_planned parenthoo...\n",
       "2      1    118     1_islamic state_new year_year eve_new year eve\n",
       "3      2    117  2_hillary clinton_bill clinton_bernie sanders_...\n",
       "4      3     81  3_academy awards_pinkett smith_jada pinkett sm...\n",
       "5      4     49  4_gun control_awr hawkins_executive gun_follow...\n",
       "6      5     35  5_breitbart news_milo yiannopoulos_breitbart t...\n",
       "7      6     30  6_hillary clinton_private server_secretary sta...\n",
       "8      7     23  7_state union_nuclear deal_january 13 2016_uni...\n",
       "9      8     23  8_natural born_born citizen_natural born citiz..."
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_model.get_topic_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "7d8050c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://intellica-ai.medium.com/comparison-of-different-word-embeddings-on-text-similarity-a-use-case-in-nlp-e83e08469c1c\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def get_cosine_similarity(feature_vec_1, feature_vec_2):\n",
    "    return cosine_similarity(feature_vec_1.reshape(1, -1), feature_vec_2.reshape(1, -1))[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "51d05d26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 36s, sys: 7.55 s, total: 1min 43s\n",
      "Wall time: 18.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#Testar distâncias diferentes\n",
    "emb = topic_model.embedding_model\n",
    "vec = emb.embedding_model.encode(np.array(docs))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "a685fdad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.05773637,  0.22130534,  0.28068772, ..., -0.02574816,\n",
       "         0.3718867 ,  0.19233046],\n",
       "       [ 0.12606996,  0.03008237,  0.06505708, ..., -0.41795808,\n",
       "         0.01794901,  0.12056918],\n",
       "       [ 0.38137358, -0.09893577, -0.29961956, ..., -0.36677417,\n",
       "        -0.29485905, -0.06710646],\n",
       "       ...,\n",
       "       [-0.24397522, -0.2544394 , -0.01839405, ...,  0.09266283,\n",
       "        -0.05090453, -0.06360812],\n",
       "       [ 0.3810828 , -0.08977187, -0.17027706, ...,  0.32137737,\n",
       "        -0.14986171,  0.15133472],\n",
       "       [-0.08440732, -0.14206052, -0.05113106, ...,  0.02968092,\n",
       "         0.00608663,  0.12419046]], dtype=float32)"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "7cc44879",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_sim_matrix = cosine_similarity(vec,vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "461b209e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bertopic.backend._sentencetransformers.SentenceTransformerBackend at 0x7f28082acca0>"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_model.embedding_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecc4b608",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c948617",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_topics_info = generate_topic_info(topic_model, i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a31cff2",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "can_execute = False\n",
    "\n",
    "if can_execute:\n",
    "    number_of_docs = 3000\n",
    "    number_of_docs_back = 1500\n",
    "    number_of_iterations = df.shape[0]//number_of_docs\n",
    "\n",
    "    start_pos = 0\n",
    "    end_pos = number_of_docs-1\n",
    "    for i in range(0, number_of_iterations):\n",
    "        if i == (number_of_iterations-1):\n",
    "            if end_pos < (df.shape[0]-1):\n",
    "                end_pos += (df.shape[0]-1) - end_pos\n",
    "\n",
    "        if i > 0:\n",
    "            df_docs = df[(start_pos-number_of_docs_back):end_pos].sort_values(by='date', ascending=True).reset_index(drop=True)\n",
    "        else:\n",
    "            df_docs = df[start_pos:end_pos].sort_values(by='date', ascending=True).reset_index(drop=True)\n",
    "\n",
    "        #if i > 12:    \n",
    "        docs = df_docs['content'].values\n",
    "        print('starting transform...')\n",
    "        topic_model = BERTopic(min_topic_size=30, language='english', calculate_probabilities=False, n_gram_range=(2,2))\n",
    "        topic_model.fit_transform(docs)\n",
    "        #print(len(topics))\n",
    "        #print(topics)\n",
    "        #break\n",
    "        topic_model.save('../raw_data/BERTopic_model_2_2_run_'+str(i))\n",
    "        print('end transform...')\n",
    "\n",
    "        df_docs.to_csv(f'../raw_data/BERTopicDocsContent_{str(i)}.csv', header=True, index=False, encoding='utf-8')\n",
    "\n",
    "        df_topics_info = generate_topic_info(topic_model, i)\n",
    "\n",
    "        df_terms = generate_terms(topic_model, i)\n",
    "\n",
    "        #df_topic_similarity = generate_topic_similarity(topic_model, i)\n",
    "\n",
    "        #df_topic_documents = generate_topic_documents(topic_model, i)\n",
    "\n",
    "        #matrix_documents_similarity = generate_documents_similarity(topic_model, docs, i)\n",
    "\n",
    "        start_pos += number_of_docs\n",
    "        end_pos += number_of_docs\n",
    "\n",
    "        docs = None\n",
    "        df_docs = None\n",
    "        df_topics_info = None\n",
    "        df_terms = None\n",
    "        df_topic_similarity = None\n",
    "        df_topic_documents = None\n",
    "        matrix_documents_similarity = None\n",
    "\n",
    "        print(f'done {str(i)} of {str(number_of_iterations-1)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53985eee",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2f926885",
   "metadata": {},
   "source": [
    "## Check term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5705582",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "number_of_iterations = 16\n",
    "\n",
    "models = []\n",
    "for i in range(0,number_of_iterations):\n",
    "    topic_model = BERTopic.load('../raw_data/BERTopic_model_2_2_run_'+str(i))\n",
    "    models.append(topic_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b40f5e3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "top_n_sim = 50\n",
    "search_terms = ['trump', 'climate change', 'biden', 'hillary']\n",
    "\n",
    "term_similarity_columns = ['mode_index', 'bert_model', 'search_term', 'topic', 'topic_docs', 'topic_start_date', 'topic_end_date', 'similarity']\n",
    "df_term_similarity = pd.DataFrame(columns=term_similarity_columns)\n",
    "\n",
    "model_ind = 0\n",
    "for topic_model in models:\n",
    "    topic_ind = -1010\n",
    "    for search_term in search_terms:\n",
    "        similar_topics, similarity = topic_model.find_topics(search_term, top_n=top_n_sim)\n",
    "        if len(similar_topics) > 0:\n",
    "            for i in range(0,len(similar_topics)):\n",
    "                if similarity[i] < 0.7:\n",
    "                    break\n",
    "                if topic_ind != similar_topics[i]:\n",
    "                    start_date, end_date, number_topic_docs = get_topic_start_end_dates(model_ind, similar_topics[i])\n",
    "                    topic_ind = similar_topics[i]\n",
    "                \n",
    "                new_term_sim = {}\n",
    "                new_term_sim['mode_index'] = model_ind\n",
    "                new_term_sim['bert_model'] = 'BERTopic_model_2_2_run_'+str(model_ind)\n",
    "                new_term_sim['search_term'] = search_term\n",
    "                new_term_sim['topic'] = similar_topics[i]\n",
    "                new_term_sim['topic_docs'] = number_topic_docs\n",
    "                new_term_sim['topic_start_date'] = start_date\n",
    "                new_term_sim['topic_end_date'] = end_date\n",
    "                new_term_sim['similarity'] = round(similarity[i],6)\n",
    "                df_term_similarity = df_term_similarity.append(new_term_sim, ignore_index=True)\n",
    "    model_ind += 1\n",
    "\n",
    "df_term_similarity = df_term_similarity.sort_values(by=['search_term', 'topic_start_date'], ascending=True).reset_index(drop=True)\n",
    "df_term_similarity.to_csv(f'../raw_data/BERTopicTermModelSimilarity.csv', header=True, index=False, encoding='utf-8')\n",
    "print(df_term_similarity.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56f11855",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_term_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58d46757",
   "metadata": {},
   "outputs": [],
   "source": [
    "# search_terms = ['trump', 'climate change', 'biden', 'hillary']\n",
    "df_term_similarity[df_term_similarity['search_term']=='climate change']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c01f02f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "parei aqui"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de283bf5",
   "metadata": {},
   "source": [
    "## Validate n_gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c027180b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dp = DataProcessor(csv_path='../raw_data/', csv_name='dataset_work')\n",
    "df = dp.load_dataset()\n",
    "df = df.sort_values(by=['year', 'month'], ascending=True).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6772917c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ = df[df['year'] == 2015].copy()\n",
    "df_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d8b624d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "n_grams = []\n",
    "n_grams.append((1,1))\n",
    "n_grams.append((1,2))\n",
    "n_grams.append((1,3))\n",
    "n_grams.append((1,4))\n",
    "n_grams.append((2,1))\n",
    "n_grams.append((2,2))\n",
    "n_grams.append((2,3))\n",
    "n_grams.append((2,4))\n",
    "n_grams.append((3,1))\n",
    "n_grams.append((3,2))\n",
    "n_grams.append((3,3))\n",
    "n_grams.append((3,4))\n",
    "n_grams.append((4,1))\n",
    "n_grams.append((4,2))\n",
    "n_grams.append((4,3))\n",
    "n_grams.append((4,4))\n",
    "for n_gram in n_grams:\n",
    "    n_gram_txt = f'{str(n_gram[0])}_{str(n_gram[1])}'\n",
    "\n",
    "    df_docs = df_.sort_values(by=['year', 'month'], ascending=True).reset_index(drop=True)\n",
    "    docs = df_docs['content'].values\n",
    "    print('starting transform...')\n",
    "    topic_model = BERTopic(min_topic_size=30, language='english', calculate_probabilities=False, n_gram_range=n_gram)\n",
    "    topic_model.fit_transform(docs)\n",
    "\n",
    "    topic_model.save('../raw_data/BERTopic_model_'+str(n_gram_txt))\n",
    "    print('end transform...')\n",
    "\n",
    "    df_docs.to_csv(f'../raw_data/BERTopicDocsContent_{n_gram_txt}.csv', header=True, index=False, encoding='utf-8')\n",
    "\n",
    "    df_topics_info = generate_topic_info(topic_model, n_gram_txt)\n",
    "    \n",
    "    df_terms = generate_terms(topic_model, n_gram_txt)\n",
    "\n",
    "    del docs\n",
    "    del df_docs\n",
    "    del df_topics_info\n",
    "    del df_terms\n",
    "\n",
    "    print(f'done n_gram: {n_gram_txt}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a12cd008",
   "metadata": {},
   "outputs": [],
   "source": [
    "icount = 0\n",
    "for n_gram in n_grams:            \n",
    "    n_gram_txt = f'{str(n_gram[0])}_{str(n_gram[1])}'\n",
    "    df_topic_info = pd.read_csv(f'../raw_data/BERTopicInfo_{n_gram_txt}.csv')\n",
    "    df_topic_info = df_topic_info.head(1).copy()\n",
    "    df_topic_info['n_gram'] = n_gram_txt\n",
    "    if icount == 0:\n",
    "        df_topic_nones = df_topic_info.copy()\n",
    "    else:\n",
    "        df_topic_nones = df_topic_nones.append(df_topic_info, ignore_index=True)\n",
    "        \n",
    "    del df_topic_info\n",
    "    icount += 1\n",
    "\n",
    "df_topic_nones.to_csv('../raw_data/BERTopicResult_n_gram.csv', header=True, index=False, encoding='utf-8')\n",
    "df_topic_nones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06a236dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_topic_nones = pd.read_csv('../raw_data/BERTopicResult_n_gram.csv')\n",
    "df_topic_nones"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
